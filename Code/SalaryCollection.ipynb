{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ae588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07bef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spotrac sign-in adapted from https://stackoverflow.com/a/60725158\n",
    "def spotracSignIn():\n",
    "    driver = webdriver.Chrome('C:/webdrivers/chromedriver.exe')\n",
    "\n",
    "    # Use Selenium to login and get all cookies \n",
    "    loginURL = 'https://www.spotrac.com/signin/'\n",
    "    username = '**********'\n",
    "    password = '**********'\n",
    "\n",
    "    driver.get(loginURL)\n",
    "\n",
    "    try:\n",
    "        # Wait for cookie message\n",
    "        accept_cookie = WebDriverWait(driver, 5, 0.25).until(EC.visibility_of_element_located([By.CSS_SELECTOR, '.cookie-alert-accept']))\n",
    "        accept_cookie.click()\n",
    "        print(\"Cookies accepted\")\n",
    "    except TimeoutException:\n",
    "        print(\"no alert\")\n",
    "\n",
    "    try:\n",
    "        # Wait for cookie message\n",
    "        popup = WebDriverWait(driver, 5, 0.25).until(EC.visibility_of_element_located([By.CSS_SELECTOR, '.cls-btn']))\n",
    "        popup.click()\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"timed out\")\n",
    "\n",
    "\n",
    "    time.sleep(5)\n",
    "    driver.find_element_by_name(\"email\").send_keys(username)\n",
    "    driver.find_element_by_name(\"password\").send_keys(password)\n",
    "\n",
    "    submit = WebDriverWait(driver, 100).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"contactForm\"]/div[2]/input')))\n",
    "    submit.click()\n",
    "    print ('Logged in!')\n",
    "    \n",
    "    return driver\n",
    "\n",
    "\n",
    "#QB CONTRACT INFO\n",
    "def getRookieQBConLen(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-length/quarterback/entry-level/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_lens = []\n",
    "        rookie = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_len = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_lens.append(con_len)\n",
    "                rookie.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Rookie\": rookie,\n",
    "            \"Con_Len\": con_lens\n",
    "        })\n",
    "        \n",
    "        df.to_csv('RookieQBConLens'+year+'.csv', index=False)\n",
    "        print(\"Created DF: RookieQBConLens\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def getVetQBConLen(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-length/quarterback/veteran/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_lens = []\n",
    "        vet = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_len = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_lens.append(con_len)\n",
    "                vet.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs, \n",
    "            \"Name\": names,\n",
    "            \"Vet\": vet,\n",
    "            \"Con_Len\": con_lens\n",
    "        })\n",
    "        \n",
    "        df.to_csv('VetQBConLens'+year+'.csv', index=False)\n",
    "        print(\"Created DF: VetQBConLens\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def getRookieQBConSal(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-value/quarterback/entry-level/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_sals = []\n",
    "        rookie = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_sal = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_sals.append(con_sal)\n",
    "                rookie.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Rookie\": rookie,\n",
    "            \"Con_Sal\": con_sals\n",
    "        })\n",
    "        \n",
    "        df.to_csv('RookieQBConSals'+year+'.csv', index=False)\n",
    "        print(\"Created DF: RookieQBConSals\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def getVetQBConSal(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-value/quarterback/veteran/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_sals = []\n",
    "        vet = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_sal = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_sals.append(con_sal)\n",
    "                vet.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Vet\": vet,\n",
    "            \"Con_Sal\": con_sals\n",
    "        })\n",
    "        \n",
    "        df.to_csv('VetQBConSals'+year+'.csv', index=False)\n",
    "        print(\"Created DF: VetQBConSals\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#RB SALARY INFO\n",
    "def getRookieRBConLen(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-length/running-back/entry-level/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_lens = []\n",
    "        rookie = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_len = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_lens.append(con_len)\n",
    "                rookie.append(1)\n",
    "                yrs.append(year)\n",
    "                \n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Rookie\": rookie,\n",
    "            \"Con_Len\": con_lens\n",
    "        })\n",
    "        \n",
    "        df.to_csv('RookieRBConLens'+year+'.csv', index=False)\n",
    "        print(\"Created DF: RookieRBConLens\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def getVetRBConLen(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-length/running-back/veteran/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_lens = []\n",
    "        vet = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_len = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_lens.append(con_len)\n",
    "                vet.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Vet\": vet,\n",
    "            \"Con_Len\": con_lens\n",
    "        })\n",
    "        \n",
    "        df.to_csv('VetRBConLens'+year+'.csv', index=False)\n",
    "        print(\"Created DF: VetRBConLens\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def getRookieRBConSal(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-value/running-back/entry-level/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_sals = []\n",
    "        rookie = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_sal = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_sals.append(con_sal)\n",
    "                rookie.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Rookie\": rookie,\n",
    "            \"Con_Sal\": con_sals\n",
    "        })\n",
    "        \n",
    "        df.to_csv('RookieRBConSals'+year+'.csv', index=False)\n",
    "        print(\"Created DF: RookieRBConSals\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def getVetRBConSal(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-value/running-back/veteran/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_sals = []\n",
    "        vet = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_sal = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_sals.append(con_sal)\n",
    "                vet.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Vet\": vet,\n",
    "            \"Con_Sal\": con_sals\n",
    "        })\n",
    "        \n",
    "        df.to_csv('VetRBConSals'+year+'.csv', index=False)\n",
    "        print(\"Created DF: VetRBConSals\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#WR SALARY INFO\n",
    "def getRookieWRConLen(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-length/wide-receiver/entry-level/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_lens = []\n",
    "        rookie = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_len = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_lens.append(con_len)\n",
    "                rookie.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Rookie\": rookie,\n",
    "            \"Con_Len\": con_lens\n",
    "        })\n",
    "        \n",
    "        df.to_csv('RookieWRConLens'+year+'.csv', index=False)\n",
    "        print(\"Created DF: RookieWRConLens\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def getVetWRConLen(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-length/wide-receiver/veteran/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_lens = []\n",
    "        vet = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_len = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_lens.append(con_len)\n",
    "                vet.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Vet\": vet,\n",
    "            \"Con_Len\": con_lens\n",
    "        })\n",
    "        \n",
    "        df.to_csv('VetWRConLens'+year+'.csv', index=False)\n",
    "        print(\"Created DF: VetWRConLens\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def getRookieWRConSal(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-value/wide-receiver/entry-level/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_sals = []\n",
    "        rookie = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_sal = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_sals.append(con_sal)\n",
    "                rookie.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs, \n",
    "            \"Name\": names,\n",
    "            \"Rookie\": rookie,\n",
    "            \"Con_Sal\": con_sals\n",
    "        })\n",
    "        \n",
    "        df.to_csv('RookieWRConSals'+year+'.csv', index=False)\n",
    "        print(\"Created DF: RookieWRConSals\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def getVetWRConSal(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-value/wide-receiver/veteran/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_sals = []\n",
    "        vet = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_sal = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_sals.append(con_sal)\n",
    "                vet.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Vet\": vet,\n",
    "            \"Con_Sal\": con_sals\n",
    "        })\n",
    "        \n",
    "        df.to_csv('VetWRConSals'+year+'.csv', index=False)\n",
    "        print(\"Created DF: VetWRConSals\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TE CONTRACT INFO\n",
    "def getRookieTEConLen(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-length/tight-end/entry-level/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_lens = []\n",
    "        rookie = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_len = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_lens.append(con_len)\n",
    "                rookie.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Rookie\": rookie,\n",
    "            \"Con_Len\": con_lens\n",
    "        })\n",
    "        \n",
    "        df.to_csv('RookieTEConLens'+year+'.csv', index=False)\n",
    "        print(\"Created DF: RookieTEConLens\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def getVetTEConLen(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-length/tight-end/veteran/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_lens = []\n",
    "        vet = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_len = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_lens.append(con_len)\n",
    "                vet.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Vet\": vet,\n",
    "            \"Con_Len\": con_lens\n",
    "        })\n",
    "        \n",
    "        df.to_csv('VetTEConLens'+year+'.csv', index=False)\n",
    "        print(\"Created DF: VetTEConLens\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def getRookieTEConSal(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-value/tight-end/entry-level/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_sals = []\n",
    "        rookie = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_sal = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_sals.append(con_sal)\n",
    "                rookie.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Rookie\": rookie,\n",
    "            \"Con_Sal\": con_sals\n",
    "        })\n",
    "        \n",
    "        df.to_csv('RookieTEConSals'+year+'.csv', index=False)\n",
    "        print(\"Created DF: RookieTEConSals\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def getVetTEConSal(startYear, endYear):\n",
    "    # Now that the cookies are there, can use requests to iterate through the links\n",
    "    url_head = \"https://www.spotrac.com/nfl/rankings/\"\n",
    "    url_tail = \"/contract-value/tight-end/veteran/\"\n",
    "    years = [str(i) for i in range(startYear, endYear+1)]\n",
    "\n",
    "    driver = spotracSignIn()\n",
    "    \n",
    "    for year in years:\n",
    "        time.sleep(2)\n",
    "        url = url_head + year + url_tail\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the table containing the quarterback information\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        table = body.find(\"table\")\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create lists to store the quarterback information\n",
    "        yrs = []\n",
    "        names = []\n",
    "        con_sals = []\n",
    "        vet = []\n",
    "\n",
    "        # Loop through the rows and extract the quarterback information\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if cells:\n",
    "                name = cells[1].find(\"a\", {\"class\":\"team-name\"}).text\n",
    "                con_sal = cells[3].find(\"span\", {\"class\":\"info\"}).text\n",
    "                names.append(name)\n",
    "                con_sals.append(con_sal)\n",
    "                vet.append(1)\n",
    "                yrs.append(year)\n",
    "\n",
    "        # Create a pandas dataframe to store the quarterback information\n",
    "        df = pd.DataFrame({\n",
    "            \"Year\": yrs,\n",
    "            \"Name\": names,\n",
    "            \"Vet\": vet,\n",
    "            \"Con_Sal\": con_sals\n",
    "        })\n",
    "        \n",
    "        df.to_csv('VetTEConSals'+year+'.csv', index=False)\n",
    "        \n",
    "        print(\"Created DF: VetTEConSals\"+year+\".csv\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d4269cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: RookieQBConLens2011.csv\n",
      "Created DF: RookieQBConLens2012.csv\n",
      "Created DF: RookieQBConLens2013.csv\n",
      "Created DF: RookieQBConLens2014.csv\n",
      "Created DF: RookieQBConLens2015.csv\n",
      "Created DF: RookieQBConLens2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: VetQBConLens2011.csv\n",
      "Created DF: VetQBConLens2012.csv\n",
      "Created DF: VetQBConLens2013.csv\n",
      "Created DF: VetQBConLens2014.csv\n",
      "Created DF: VetQBConLens2015.csv\n",
      "Created DF: VetQBConLens2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: RookieQBConSals2011.csv\n",
      "Created DF: RookieQBConSals2012.csv\n",
      "Created DF: RookieQBConSals2013.csv\n",
      "Created DF: RookieQBConSals2014.csv\n",
      "Created DF: RookieQBConSals2015.csv\n",
      "Created DF: RookieQBConSals2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: VetQBConSals2011.csv\n",
      "Created DF: VetQBConSals2012.csv\n",
      "Created DF: VetQBConSals2013.csv\n",
      "Created DF: VetQBConSals2014.csv\n",
      "Created DF: VetQBConSals2015.csv\n",
      "Created DF: VetQBConSals2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: RookieRBConLens2011.csv\n",
      "Created DF: RookieRBConLens2012.csv\n",
      "Created DF: RookieRBConLens2013.csv\n",
      "Created DF: RookieRBConLens2014.csv\n",
      "Created DF: RookieRBConLens2015.csv\n",
      "Created DF: RookieRBConLens2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: VetRBConLens2011.csv\n",
      "Created DF: VetRBConLens2012.csv\n",
      "Created DF: VetRBConLens2013.csv\n",
      "Created DF: VetRBConLens2014.csv\n",
      "Created DF: VetRBConLens2015.csv\n",
      "Created DF: VetRBConLens2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: RookieRBConSals2011.csv\n",
      "Created DF: RookieRBConSals2012.csv\n",
      "Created DF: RookieRBConSals2013.csv\n",
      "Created DF: RookieRBConSals2014.csv\n",
      "Created DF: RookieRBConSals2015.csv\n",
      "Created DF: RookieRBConSals2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: VetRBConSals2011.csv\n",
      "Created DF: VetRBConSals2012.csv\n",
      "Created DF: VetRBConSals2013.csv\n",
      "Created DF: VetRBConSals2014.csv\n",
      "Created DF: VetRBConSals2015.csv\n",
      "Created DF: VetRBConSals2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: RookieWRConLens2011.csv\n",
      "Created DF: RookieWRConLens2012.csv\n",
      "Created DF: RookieWRConLens2013.csv\n",
      "Created DF: RookieWRConLens2014.csv\n",
      "Created DF: RookieWRConLens2015.csv\n",
      "Created DF: RookieWRConLens2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: VetWRConLens2011.csv\n",
      "Created DF: VetWRConLens2012.csv\n",
      "Created DF: VetWRConLens2013.csv\n",
      "Created DF: VetWRConLens2014.csv\n",
      "Created DF: VetWRConLens2015.csv\n",
      "Created DF: VetWRConLens2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: RookieWRConSals2011.csv\n",
      "Created DF: RookieWRConSals2012.csv\n",
      "Created DF: RookieWRConSals2013.csv\n",
      "Created DF: RookieWRConSals2014.csv\n",
      "Created DF: RookieWRConSals2015.csv\n",
      "Created DF: RookieWRConSals2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: VetWRConSals2011.csv\n",
      "Created DF: VetWRConSals2012.csv\n",
      "Created DF: VetWRConSals2013.csv\n",
      "Created DF: VetWRConSals2014.csv\n",
      "Created DF: VetWRConSals2015.csv\n",
      "Created DF: VetWRConSals2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: RookieTEConLens2011.csv\n",
      "Created DF: RookieTEConLens2012.csv\n",
      "Created DF: RookieTEConLens2013.csv\n",
      "Created DF: RookieTEConLens2014.csv\n",
      "Created DF: RookieTEConLens2015.csv\n",
      "Created DF: RookieTEConLens2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: VetTEConLens2011.csv\n",
      "Created DF: VetTEConLens2012.csv\n",
      "Created DF: VetTEConLens2013.csv\n",
      "Created DF: VetTEConLens2014.csv\n",
      "Created DF: VetTEConLens2015.csv\n",
      "Created DF: VetTEConLens2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: RookieTEConSals2011.csv\n",
      "Created DF: RookieTEConSals2012.csv\n",
      "Created DF: RookieTEConSals2013.csv\n",
      "Created DF: RookieTEConSals2014.csv\n",
      "Created DF: RookieTEConSals2015.csv\n",
      "Created DF: RookieTEConSals2016.csv\n",
      "no alert\n",
      "timed out\n",
      "Logged in!\n",
      "Created DF: VetTEConSals2011.csv\n",
      "Created DF: VetTEConSals2012.csv\n",
      "Created DF: VetTEConSals2013.csv\n",
      "Created DF: VetTEConSals2014.csv\n",
      "Created DF: VetTEConSals2015.csv\n",
      "Created DF: VetTEConSals2016.csv\n"
     ]
    }
   ],
   "source": [
    "getRookieQBConLen(2011, 2016)\n",
    "time.sleep(10)\n",
    "getVetQBConLen(2011, 2016)\n",
    "time.sleep(10)\n",
    "getRookieQBConSal(2011, 2016)\n",
    "time.sleep(10)\n",
    "getVetQBConSal(2011, 2016)\n",
    "time.sleep(10)\n",
    "\n",
    "getRookieRBConLen(2011, 2016)\n",
    "time.sleep(10)\n",
    "getVetRBConLen(2011, 2016)\n",
    "time.sleep(10)\n",
    "getRookieRBConSal(2011, 2016)\n",
    "time.sleep(10)\n",
    "getVetRBConSal(2011, 2016)\n",
    "time.sleep(10)\n",
    "\n",
    "getRookieWRConLen(2011, 2016)\n",
    "time.sleep(10)\n",
    "getVetWRConLen(2011, 2016)\n",
    "time.sleep(10)\n",
    "getRookieWRConSal(2011, 2016)\n",
    "time.sleep(10)\n",
    "getVetWRConSal(2011, 2016)\n",
    "time.sleep(10)\n",
    "\n",
    "getRookieTEConLen(2011, 2016)\n",
    "time.sleep(10)\n",
    "getVetTEConLen(2011, 2016)\n",
    "time.sleep(10)\n",
    "getRookieTEConSal(2011, 2016)\n",
    "time.sleep(10)\n",
    "getVetTEConSal(2011, 2016)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581df7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
